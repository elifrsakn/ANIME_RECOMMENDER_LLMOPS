# ANIME_RECOMMENDER_LLMOPS

An anime recommendation system powered by LLMs, containerized with Docker, deployed via Kubernetes, and built with a production-ready MLOps pipeline.

## ğŸ“Œ Project Overview

This project builds an end-to-end **Large Language Model (LLM)**-based anime recommendation system.  
It takes user prompts, processes them through a language model, and provides meaningful anime suggestions.  
The project includes a modular architecture and is designed for **production deployment** using **Docker, Kubernetes, and GCP**.

---

## ğŸ§± Project Structure

```bash
â”œâ”€â”€ app/                    # Streamlit frontend app
â”œâ”€â”€ chroma_db/             # Local vector DB setup using Chroma
â”œâ”€â”€ config/                # Configuration files
â”œâ”€â”€ data/                  # Datasets
â”œâ”€â”€ pipeline/              # Training and recommendation pipeline
â”œâ”€â”€ src/                   # Core logic (loader, prompt handler, recommender)
â”œâ”€â”€ utils/                 # Utility functions
â”œâ”€â”€ dockerfile             # Docker image definition
â”œâ”€â”€ llmops-k8s.yaml        # Kubernetes deployment configuration
â”œâ”€â”€ requirements.txt       # Python dependencies
â””â”€â”€ test.py                # Test script
```

---

## ğŸ§  Technologies Used

### ğŸ¤– LLM & Prompt Engineering
- [Groq API](https://groq.com/) â€” High-performance LLM inference
- [Hugging Face API](https://huggingface.co/) â€” Model access
- Custom Prompt Templates â€” Tailored prompts for better recommendations

### ğŸ§ª Data Pipeline
- Modular `DataLoader`, `PromptHandler`, and `AnimeRecommender` classes
- Vector embedding storage via **ChromaDB**

### ğŸ³ Containerization
- Fully containerized using Docker
- Dockerfile for reproducible builds

### â˜¸ï¸ Kubernetes Deployment
- `llmops-k8s.yaml` for K8s deployment
- Tested with **Minikube** and **GCP**
- Managed using `kubectl`

### â˜ï¸ GCP (Google Cloud Platform)
- Hosted on a GCP VM instance
- Firewall and access rules configured for Kubernetes cluster

### ğŸ”„ CI/CD & Version Control
- GitHub used for version control and integration
- Environment configurations and deployment specs are tracked

---

## ğŸ“Š Monitoring (Coming Soon)

Monitoring stack to be integrated:

- **Grafana Cloud** â€” For real-time Kubernetes cluster metrics
- **Prometheus** â€” For metrics collection and alerting
- Track pod performance, latency, memory/CPU usage, and error logs

ğŸ”§ *This section will be updated once monitoring is fully set up.*

---

## âš™ï¸ Installation

```bash
# Create virtual environment
python -m venv venv
source venv/bin/activate

# Install dependencies
pip install -r requirements.txt
```

---

## ğŸ³ Docker Usage

```bash
# Build Docker image
docker build -t anime-recommender .

# Run container
docker run -p 8501:8501 anime-recommender
```

---

## â˜¸ï¸ Kubernetes Deployment

```bash
# Deploy using kubectl
kubectl apply -f llmops-k8s.yaml

# Check running pods
kubectl get pods

# View logs
kubectl logs <pod-name>
```

---

## ğŸ“ Roadmap

- [x] HuggingFace & Groq API Integration
- [x] ChromaDB vector DB
- [x] Docker containerization
- [x] Kubernetes deployment
- [x] GCP instance setup
- [ ] Monitoring with Grafana + Prometheus

---

## ğŸ‘©â€ğŸ’» Developer

- [Elif Sakin](https://github.com/sakinnn) â€” AI / MLOps Developer
---

